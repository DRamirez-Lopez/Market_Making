{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Documentación — Agente de Market Making con RL\n",
        "\n",
        "**Stack:** Gymnasium · Stable-Baselines3 (PPO) · Python 3.10+\n",
        "\n",
        "**Archivo principal:** `avellaneda_stoikov_gym_env.py`\n",
        "\n",
        "---\n",
        "\n",
        "## 0) Resumen ejecutivo (paso a paso breve)\n",
        "1. **Crear entorno** (Conda recomendado):\n",
        "   ```bash\n",
        "   conda create -n mmrl python=3.10 -y\n",
        "   conda activate mmrl\n",
        "   python -m pip install --upgrade pip\n",
        "   pip install gymnasium numpy stable-baselines3\n",
        "   pip install \"torch>=2.2,<3.0\" --index-url https://download.pytorch.org/whl/cpu\n",
        "   ```\n",
        "2. **Guardar** `avellaneda_stoikov_gym_env.py` en tu carpeta de trabajo.\n",
        "3. **Entrenar** (siempre con el Python del entorno):\n",
        "   ```bash\n",
        "   python .\\avellaneda_stoikov_gym_env.py\n",
        "   ```\n",
        "4. **Ver resultados**: la consola imprimirá métricas de SB3 durante el entrenamiento y, al final, un **rollout de evaluación** con **Total reward** y **Final PnL**.\n",
        "5. **Modificar parámetros** del entorno (volatilidad, intensidades, penalización de inventario) y de PPO para experimentar con el desempeño.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Objetivo y diseño del agente\n",
        "Construir un **market maker** que cotiza **bid/ask** y gestiona inventario en un libro de órdenes, maximizando PnL con control de riesgo. El entorno implementa una versión didáctica del modelo **Avellaneda–Stoikov**:\n",
        "\n",
        "- **Precio de reserva**: $ r_t = s_t - q_t\\,\\gamma\\,\\sigma^2\\,(T-t) $, que desplaza las cotizaciones para reducir inventario.\n",
        "- **Llegadas de órdenes**: procesos de Poisson con **intensidad exponencial** decreciente con la distancia a mid: $\\lambda(\\delta) = A e^{-k\\delta}$.\n",
        "- **Acción del agente**: $ (\\text{skew},\\ \\text{half_spread}) $ (ambos en dólares, con ticks).\n",
        "- **Recompensa**: $ \\Delta\\text{PnL} - \\text{inv_penalty}\\cdot q_t^2\\cdot dt $ (shaping por inventario).\n",
        "\n",
        "> **Nota:** Es un simulador base para investigación y docencia. Para uso productivo, ver 7.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Estructura del entorno (Gymnasium)\n",
        "- **Observación** (vector de 5):\n",
        "  1) `mid_price_norm`, 2) `inventory_norm`, 3) `time_remaining`, 4) `last_halfspread_norm`, 5) `last_skew_norm`.\n",
        "- **Acción**: `Box(2,)` → `(skew, half_spread)` con límites y tick.\n",
        "- **Dinámica del precio**: Browniano aritmético $ s_{t+dt} = s_t + \\sigma\\sqrt{dt} Z $.\n",
        "- **Ejecuciones**: probabilidad de fill por lado en $[t, t+dt] ≈  1 - e^{-\\lambda(\\delta)\\,dt} $. Un fill por lado por paso (unitario, configurable).\n",
        "- **Terminal**: `t >= T` o `|q| >= max_inv`.\n",
        "- **`info`**: `mid_price`, `cash`, `inventory`, `bid`, `ask`, `pnl`, `time`.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Parámetros clave\n",
        "### 3.1 Configuración del entorno (`EnvConfig`)\n",
        "| Parámetro | Significado | Valor por defecto |\n",
        "|---|---|---|\n",
        "| `T` | Horizonte del episodio (días de mercado) | `1.0` |\n",
        "| `dt` | Paso temporal (fracción del día) | `1/390/10` (≈ 6 seg) |\n",
        "| `sigma` | Volatilidad diaria (abs) | `2.0` |\n",
        "| `s0` | Mid inicial | `100.0` |\n",
        "| `gamma` | Aversión al riesgo (AS) | `0.1` |\n",
        "| `A` | Nivel base de llegadas | `140.0` |\n",
        "| `k` | Pendiente de intensidad vs. distancia | `1.5` |\n",
        "| `tick_size` | Tamaño de tick | `0.01` |\n",
        "| `max_inv` | Límite de inventario | `50` |\n",
        "| `inv_penalty` | Penalización de inventario | `0.02` |\n",
        "| `max_half_spread` | Cota superior del half-spread | `1.5` |\n",
        "| `max_skew` | Cota superior del skew | `1.5` |\n",
        "| `fill_size` | Tamaño por fill | `1` |\n",
        "\n",
        "**Guidelines de tuning:**\n",
        "- Aumenta `sigma` → más riesgo direccional; el agente debería ampliar spreads y recentrar más agresivamente.\n",
        "- Aumenta `A` o reduce `k` → más fills a igual distancia; spreads pueden estrecharse.\n",
        "- Sube `gamma` y/o `inv_penalty` → el agente prioriza descargar inventario (menor varianza de PnL, quizá menor retorno).\n",
        "\n",
        "### 3.2 Hiperparámetros PPO (SB3)\n",
        "| Parámetro | Valor | Comentario |\n",
        "|---|---:|---|\n",
        "| `n_steps` | 2048 | Trajectorias por update (↑ → gradientes más estables) |\n",
        "| `batch_size` | 256 | Tamaño de lote para SGD |\n",
        "| `gamma` | 0.999 | Horizonte largo (adecuado para PnL acumulado) |\n",
        "| `gae_lambda` | 0.95 | Generalized Advantage Estimation |\n",
        "| `learning_rate` | 3e-4 | Tasa de aprendizaje |\n",
        "| `clip_range` | 0.2 | Clipping de PPO |\n",
        "| `ent_coef` | 0.01 | Entropía → fomenta exploración |\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Cómo se construye el agente (paso a paso)\n",
        "1. **Modelar microestructura mínima**: define mid-price (Browniano), función de intensidades y constraints (tick, inventario, tamaños).\n",
        "2. **Definir la observación**: normaliza precio/inventario y añade features de control (tiempo restante, acción anterior) para estabilidad.\n",
        "3. **Definir la acción**: `(skew, half_spread)` acotados; redondea a tick y asegura `ask - bid ≥ 2*tick_size`.\n",
        "4. **Cálculo de cotizaciones**: centra en **precio de reserva** y aplica `(skew, half_spread)` → `bid`, `ask`.\n",
        "5. **Simular ejecuciones**: aproxima rellenos por lado con Poisson y prob. de fill en `dt`; actualiza `cash`/`q`.\n",
        "6. **Evolución del precio**: avanza `s` con \\(\\sigma\\sqrt{dt}Z\\).\n",
        "7. **Recompensa**: PnL marcado a mercado y penalización por inventario.\n",
        "8. **Término del episodio**: por tiempo o límite de inventario.\n",
        "9. **Entrenamiento PPO**: env + vectorizado (si quieres), ciclo `learn()` con millones de timesteps.\n",
        "10. **Evaluación**: rollout determinístico → `Total reward`, `Final PnL`; repetir en múltiples seeds/episodios.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Uso e interpretación\n",
        "### 5.1 Ejecución\n",
        "```bash\n",
        "python .\\avellaneda_stoikov_gym_env.py\n",
        "```\n",
        "- Verás logs periódicos de SB3 (`ep_len_mean`, `ep_rew_mean`, `explained_variance`, `entropy_loss`, etc.).\n",
        "- Al final: `Episode finished. Total reward: ...  Final PnL: ...`.\n",
        "\n",
        "### 5.2 Métricas a vigilar\n",
        "- **`ep_rew_mean`**: hacia ↑ con entrenamiento (ojo si se estanca en negativo).\n",
        "- **`explained_variance`**: ↑ indica que la red de valor está aprendiendo la dinámica de recompensas.\n",
        "- **`approx_kl`** y **`clip_fraction`**: cambios de política razonables (ni 0 ni demasiado altos).\n",
        "- **`std`** (Gauss policy): debería bajar paulatinamente (menos exploración).\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Baselines y experimentos\n",
        "- **Heurística AS** (spread cerrado-forma):\n",
        "  \\[ \\delta_b = \\gamma q\\sigma^2(T-t) + \\frac{1}{\\gamma}\\ln\\big(1+\\tfrac{\\gamma}{k}\\big),\\quad\n",
        "     \\delta_a = -\\gamma q\\sigma^2(T-t) + \\frac{1}{\\gamma}\\ln\\big(1+\\tfrac{\\gamma}{k}\\big) \\]\n",
        "  Comparar RL vs. esta política y vs. **estrategia simétrica** (mismo spread centrado en mid).\n",
        "- **Ablation**: variar `gamma`, `inv_penalty`, `A`, `k` y medir impacto en PnL/volatilidad de inventario.\n",
        "- **Curriculum**: empezar con baja `sigma`/alto `A` e ir endureciendo.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Hacia un entorno más realista\n",
        "Para pruebas serias:\n",
        "- **Fees y rebates** (maker/taker), **latencia** (place/cancel), **fills parciales y cola FIFO**.\n",
        "- **Replay L2/L3**: reproducir días históricos con prioridad de cola y barridos; evaluar por regímenes (alta/baja vol).\n",
        "- **Marcación**: mid vs. last vs. VWAP; *slippage* si cruzas.\n",
        "- **Risk checks**: límites de notional, `|q|`, kill-switch por drawdown.\n",
        "\n",
        "> Reemplaza las \\(\\lambda(\\delta)\\) teóricas por **tablas empíricas** de \\(P(\\text{fill}|\\delta, \\text{spread}, \\text{vol}, t)\\).\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Buenas prácticas\n",
        "- **Semillas** y múltiples rollouts para medias/intervalos de confianza.\n",
        "- **Walk-forward** por fechas;\n",
        "- **Normalización de features** estable (evitar fugas de información).\n",
        "- **Logging estructurado** (TensorBoard / CSV) + trazas de trades.\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Solución de problemas frecuentes (Windows/Conda)\n",
        "- `ModuleNotFoundError: gymnasium` → ejecutar siempre con `python` del entorno, no `py`.\n",
        "- `Python < 3.8` → crear entorno nuevo `python=3.10`.\n",
        "- Torch en CPU/GPU → instalar wheel acorde a tu hardware.\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Snippets útiles\n",
        "**Cambiar parámetros del entorno** en el ejemplo de `__main__`:\n",
        "```python\n",
        "from avellaneda_stoikov_gym_env import EnvConfig, make_env\n",
        "cfg = EnvConfig(sigma=3.0, A=200.0, k=1.2, inv_penalty=0.03, max_inv=30)\n",
        "env = make_env(cfg, seed=42)\n",
        "```\n",
        "\n",
        "**Guardar y cargar el modelo PPO**:\n",
        "```python\n",
        "model.save(\"ppo_mm.zip\")\n",
        "# ...\n",
        "from stable_baselines3 import PPO\n",
        "model = PPO.load(\"ppo_mm.zip\", env=env, device=\"auto\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 11) Referencias sugeridas\n",
        "- Avellaneda, M., & Stoikov, S. (2008). *High-frequency trading in a limit order book*.\n",
        "- Ho, T., & Stoll, H. (1981, 1983). *Optimal dealer pricing*.\n",
        "- O’Hara, M. (1995). *Market Microstructure Theory*.\n",
        "\n",
        "---\n",
        "\n",
        "**Contacto/Notas**: Ajusta los parámetros a tu activo y franja horaria. Para “production”, prioriza el §7 (realismo) y una batería de tests fuera de muestra.\n",
        "\n"
      ],
      "metadata": {
        "id": "YV6NA61tYsmK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOUTrxDaQlce"
      },
      "outputs": [],
      "source": [
        "  \"\"\"\n",
        "Avellaneda–Stoikov Market Making Gymnasium Environment (RL-ready)\n",
        "-----------------------------------------------------------------\n",
        "This module implements a self-contained reinforcement-learning environment for\n",
        "market making inspired by Avellaneda & Stoikov (2008). It uses the Gymnasium API,\n",
        "so you can plug it into SB3 / CleanRL / Ray RLlib.\n",
        "\n",
        "Key features\n",
        "- Mid-price follows arithmetic Brownian motion with volatility sigma.\n",
        "- Order arrivals follow Poisson processes with exponential intensity vs. quote distance.\n",
        "- Agent places ONE unit bid/ask per step by choosing (skew, half_spread).\n",
        "- Reservation price from AS is used as a baseline (risk-aversion gamma).\n",
        "- Reward = ΔPnL − inv_penalty * q^2 * dt (inventory risk shaping).\n",
        "\n",
        "Usage\n",
        "-----\n",
        "$ pip install gymnasium numpy stable-baselines3\n",
        "\n",
        "from avellaneda_stoikov_gym_env import MarketMakingEnv, make_env\n",
        "env = make_env(seed=0)\n",
        "obs, info = env.reset()\n",
        "obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
        "\n",
        "Training with SB3 (see __main__ block at bottom for a runnable example):\n",
        "$ python avellaneda_stoikov_gym_env.py\n",
        "\n",
        "Notes\n",
        "-----\n",
        "- Prices are in arbitrary units; you should calibrate sigma, A, k, tick_size to your asset.\n",
        "- This is a pedagogical baseline. Realistic fills (queue position, partials),\n",
        "  discrete ticks, and adverse selection models can be added later.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EnvConfig:\n",
        "    T: float = 1.0                  # episode horizon in (trading) days\n",
        "    dt: float = 1/390.0/10.0        # step ~ 1/10 of a minute in a 6.5h session\n",
        "    sigma: float = 2.0              # mid-price vol (abs units per sqrt(day))\n",
        "    s0: float = 100.0               # initial mid-price\n",
        "    gamma: float = 0.1              # risk aversion (AS)\n",
        "    A: float = 140.0                # base arrival intensity level (per day)\n",
        "    k: float = 1.5                  # slope of intensity vs. distance\n",
        "    tick_size: float = 0.01         # price tick\n",
        "    max_inv: int = 50               # inventory hard cap (units)\n",
        "    inv_penalty: float = 0.02       # quadratic inventory penalty coefficient\n",
        "    max_half_spread: float = 1.5    # action bound (absolute dollars)\n",
        "    max_skew: float = 1.5           # action bound (absolute dollars)\n",
        "    fill_size: int = 1              # units per fill\n",
        "    seed: int | None = None\n",
        "\n",
        "\n",
        "class MarketMakingEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Observation: (5,)\n",
        "      [ mid_price_norm, inventory_norm, time_remaining, last_halfspread_norm, last_skew_norm ]\n",
        "    Action: (2,) Box\n",
        "      action[0] = skew adjustment (dollars) around reservation price (positive -> shift quotes downward)\n",
        "      action[1] = half_spread (dollars, >= tick_size)\n",
        "    Reward:\n",
        "      ΔPnL − inv_penalty * q^2 * dt (shaping)\n",
        "    Episode ends at t>=T or |q|>=max_inv (truncated if inventory cap hit)\n",
        "    \"\"\"\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 60}\n",
        "\n",
        "    def __init__(self, cfg: EnvConfig = EnvConfig()):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.rng = np.random.default_rng(cfg.seed)\n",
        "\n",
        "        # Spaces\n",
        "        low = np.array([-cfg.max_skew, cfg.tick_size], dtype=np.float32)\n",
        "        high = np.array([cfg.max_skew, cfg.max_half_spread], dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=low, high=high, shape=(2,), dtype=np.float32)\n",
        "\n",
        "        # Observations are roughly standardized\n",
        "        self._price_scale = max(cfg.s0, 1.0)\n",
        "        self._spread_scale = max(cfg.max_half_spread, cfg.tick_size)\n",
        "        self._skew_scale = max(cfg.max_skew, cfg.tick_size)\n",
        "        obs_low = np.array([0.0, -1.0, 0.0, 0.0, -1.0], dtype=np.float32)\n",
        "        obs_high = np.array([np.inf, 1.0, 1.0, 1.0, 1.0], dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
        "\n",
        "        # State vars\n",
        "        self.reset(seed=cfg.seed)\n",
        "\n",
        "    # ---- Core dynamics helpers -------------------------------------------------\n",
        "    def _reservation_price(self, s: float, q: int, t: float) -> float:\n",
        "        # r = s − q * gamma * sigma^2 * (T − t)  (Avellaneda–Stoikov heuristic)\n",
        "        return s - q * self.cfg.gamma * (self.cfg.sigma ** 2) * (self.cfg.T - t)\n",
        "\n",
        "    def _intensity(self, delta: float) -> float:\n",
        "        # λ(δ) = A * exp(−k * δ); clamp delta>=0 for intensity definition\n",
        "        return self.cfg.A * math.exp(-self.cfg.k * max(0.0, delta))\n",
        "\n",
        "    def _step_price(self, s: float) -> float:\n",
        "        # Arithmetic BM step: s_{t+dt} = s + sigma * sqrt(dt) * Z\n",
        "        z = self.rng.standard_normal()\n",
        "        return s + self.cfg.sigma * math.sqrt(self.cfg.dt) * z\n",
        "\n",
        "    # ---- Gym API ---------------------------------------------------------------\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        mid_price_norm = self.s / self._price_scale\n",
        "        inventory_norm = np.clip(self.q / self.cfg.max_inv, -1.0, 1.0)\n",
        "        time_remaining = max(self.cfg.T - self.t, 0.0) / self.cfg.T\n",
        "        last_halfspread_norm = self.last_half_spread / self._spread_scale\n",
        "        last_skew_norm = np.clip(self.last_skew / self._skew_scale, -1.0, 1.0)\n",
        "        return np.array([\n",
        "            mid_price_norm,\n",
        "            inventory_norm,\n",
        "            time_remaining,\n",
        "            last_halfspread_norm,\n",
        "            last_skew_norm,\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def _get_info(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"mid_price\": self.s,\n",
        "            \"cash\": self.cash,\n",
        "            \"inventory\": int(self.q),\n",
        "            \"bid\": self.bid,\n",
        "            \"ask\": self.ask,\n",
        "            \"pnl\": self.cash + self.q * self.s,\n",
        "            \"time\": self.t,\n",
        "        }\n",
        "\n",
        "    def reset(self, *, seed: int | None = None, options: Dict[str, Any] | None = None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "        self.s = float(self.cfg.s0)\n",
        "        self.q = 0\n",
        "        self.cash = 0.0\n",
        "        self.t = 0.0\n",
        "        self.last_half_spread = self.cfg.tick_size\n",
        "        self.last_skew = 0.0\n",
        "        self.bid = self.s - self.last_half_spread\n",
        "        self.ask = self.s + self.last_half_spread\n",
        "        obs = self._get_obs()\n",
        "        info = self._get_info()\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:\n",
        "        skew, half_spread = float(action[0]), float(action[1])\n",
        "        # Bound actions explicitly\n",
        "        half_spread = float(np.clip(half_spread, self.cfg.tick_size, self.cfg.max_half_spread))\n",
        "        skew = float(np.clip(skew, -self.cfg.max_skew, self.cfg.max_skew))\n",
        "\n",
        "        # Baseline reservation price and quotes (ticks enforced)\n",
        "        r = self._reservation_price(self.s, self.q, self.t)\n",
        "        bid = r - half_spread - skew\n",
        "        ask = r + half_spread - skew\n",
        "        # Tick rounding\n",
        "        bid = math.floor(bid / self.cfg.tick_size) * self.cfg.tick_size\n",
        "        ask = math.ceil(ask / self.cfg.tick_size) * self.cfg.tick_size\n",
        "        # Ensure bid < ask\n",
        "        if ask - bid < 2 * self.cfg.tick_size:\n",
        "            # widen minimally to respect ticks\n",
        "            mid = 0.5 * (bid + ask)\n",
        "            bid = mid - self.cfg.tick_size\n",
        "            ask = mid + self.cfg.tick_size\n",
        "\n",
        "        # Execution probabilities over dt (at most one fill each side per step)\n",
        "        delta_b = max(self.s - bid, 0.0)\n",
        "        delta_a = max(ask - self.s, 0.0)\n",
        "        lam_b = self._intensity(delta_b)\n",
        "        lam_a = self._intensity(delta_a)\n",
        "        p_fill_b = 1.0 - math.exp(-lam_b * self.cfg.dt)\n",
        "        p_fill_a = 1.0 - math.exp(-lam_a * self.cfg.dt)\n",
        "\n",
        "        # Sample fills\n",
        "        fill_b = self.rng.random() < p_fill_b  # we BUY at bid\n",
        "        fill_a = self.rng.random() < p_fill_a  # we SELL at ask\n",
        "\n",
        "        # Apply fills (one unit each side)\n",
        "        cash_before = self.cash\n",
        "        q_before = self.q\n",
        "        if fill_b:\n",
        "            self.cash -= bid * self.cfg.fill_size\n",
        "            self.q += self.cfg.fill_size\n",
        "        if fill_a:\n",
        "            self.cash += ask * self.cfg.fill_size\n",
        "            self.q -= self.cfg.fill_size\n",
        "\n",
        "        # Price evolution after orders\n",
        "        s_prev = self.s\n",
        "        self.s = self._step_price(self.s)\n",
        "\n",
        "        # Reward: mark-to-market PnL change minus inventory penalty\n",
        "        pnl_prev = cash_before + q_before * s_prev\n",
        "        pnl_now = self.cash + self.q * self.s\n",
        "        pnl_delta = pnl_now - pnl_prev\n",
        "        inv_pen = self.cfg.inv_penalty * (self.q ** 2) * self.cfg.dt\n",
        "        reward = float(pnl_delta - inv_pen)\n",
        "\n",
        "        # Advance time and write state\n",
        "        self.t += self.cfg.dt\n",
        "        self.last_half_spread = half_spread\n",
        "        self.last_skew = skew\n",
        "        self.bid, self.ask = bid, ask\n",
        "\n",
        "        # Termination / Truncation\n",
        "        terminated = self.t >= self.cfg.T\n",
        "        truncated = abs(self.q) >= self.cfg.max_inv\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        info = self._get_info()\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    # Optional\n",
        "    def render(self):\n",
        "        print(self._get_info())\n",
        "\n",
        "\n",
        "# Convenience factory\n",
        "\n",
        "def make_env(cfg: EnvConfig | None = None, seed: int | None = None) -> MarketMakingEnv:\n",
        "    cfg = cfg or EnvConfig()\n",
        "    if seed is not None:\n",
        "        cfg.seed = seed\n",
        "    return MarketMakingEnv(cfg)\n",
        "\n",
        "\n",
        "# --------------------------- Training Example (SB3) ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        from stable_baselines3 import PPO\n",
        "        from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "        from stable_baselines3.common.monitor import Monitor\n",
        "    except Exception as e:\n",
        "        print(\"Stable-Baselines3 not available. Install with: pip install stable-baselines3\")\n",
        "        raise\n",
        "\n",
        "    # Create and wrap env\n",
        "    def _factory():\n",
        "        env = make_env(seed=42)\n",
        "        return Monitor(env)\n",
        "\n",
        "    vec_env = DummyVecEnv([_factory])\n",
        "\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        vec_env,\n",
        "        verbose=1,\n",
        "        n_steps=2048,\n",
        "        batch_size=256,\n",
        "        gae_lambda=0.95,\n",
        "        gamma=0.999,          # long-ish horizon\n",
        "        learning_rate=3e-4,\n",
        "        clip_range=0.2,\n",
        "        ent_coef=0.01,\n",
        "        tensorboard_log=None,\n",
        "        device=\"auto\",\n",
        "    )\n",
        "\n",
        "    timesteps = 200_000\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "\n",
        "    # Quick evaluation rollout\n",
        "    env = make_env(seed=123)\n",
        "    obs, info = env.reset()\n",
        "    total_reward = 0.0\n",
        "    while True:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    final_pnl = info[\"pnl\"]\n",
        "    print(f\"Episode finished. Total reward: {total_reward:.2f}  Final PnL: {final_pnl:.2f}\")\n"
      ]
    }
  ]
}